{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 1 \"Полносвязные нейронные сети\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ФИО: Михайлин Дмитрий Александрович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит реализовать проход \"вперед\" для полносвязной нейронную сети. В дальнейшем мы реализуем процедуру обучения и научим сеть распознавать рукописные цифры.\n",
    "\n",
    "На первой лекции мы познакомились с тем, что такое нейронные сети и изучили три слоя — линейный, сигмоида и SoftMax. Из этих слоев можно составлять глубокие архитектуры и обучать их при помощи градиентного спуска. Чтобы конструировать сложные архитектуры, можно реализовать каждый тип слоя как отдельный \"кирпичик\" и затем собирать полную архитектуру как конструктор. Это мы и попробуем сделать на первом и втором семинарах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый тип слоя мы будем реализовывать при помощи класса, который будет поддерживать три функции: forward, которая будет применять функцию, реализуемую слоем, к входной матрице и backward, которая будет вычислять градиенты и step, которая будет обновлять веса. Чтобы не применять функцию к каждому объекту в отдельности, мы будем подавать на вход слою матрицу размера (N, d), где N — количество объектов, а d — размерность каждого объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=backprop.pdf width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция forward будет вычислять по $x$ значение $y$, backward — по $\\frac{\\partial L}{\\partial y}$ вычислять $\\frac{\\partial L}{\\partial x}$ и обновлять внутри себя $\\frac{\\partial L}{\\partial w}$.\n",
    "\n",
    "Важным требованием к реализации является векторизация всех слоев: все операции должны быть сведены к матричным, не должно быть циклов. Это значительно уменьшает временные затраты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: Линейный слой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем пример вычисления градиентов для линейного слоя: $y = Wx$, $x \\in \\mathbb{R}^{K \\times n}$, $y \\in \\mathbb{R}^{K \\times n}$, $W \\in \\mathbb{R}^{n \\times m}$, где $K$ — число объектов.\n",
    "\n",
    "Рассмотрим $L$ как функцию от выходов нейронной сети: $L = L(y_{11}, y_{12}, \\dots)$\n",
    "\n",
    "$$y_{kt} = (Wx)_{kt} = \\sum_{z=1}^{n} x_{kz}W_{zt}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_{ij}} = \\sum_{kt} \\frac{\\partial L}{\\partial y_{kt}}\\frac{\\partial y_{kt}}{\\partial x_{ij}} = \\sum_{kt} \\frac{\\partial L}{\\partial y_{kt}}\\frac{\\partial \\sum_z x_{kz}w_{zt}}{\\partial x_{ij}}= \\sum_{t} \\frac{\\partial L}{\\partial y_{it}}\\frac{\\partial w_{jt}}{\\partial x_{ij}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial x} = \\frac{\\partial{L}}{\\partial y}W^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Creates weights and biases for linear layer.\n",
    "        Dimention of inputs is *input_size*, of output: *output_size*.\n",
    "        '''\n",
    "        self.W = np.random.normal(0, 0.1, (input_size, output_size))\n",
    "        self.b = np.zeros(output_size)\n",
    "        #### YOUR CODE HERE\n",
    "        #### Create weights, initialize them with samples from N(0, 0.1).\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, input_size).\n",
    "        Returns output of size (N, output_size).\n",
    "        Hint: You may need to store X for backward pass\n",
    "        '''\n",
    "        self.X = X\n",
    "        self.Y = X @ self.W + self.b\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layer to input\n",
    "        return self.Y\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdw and dLdx.\n",
    "        2. Store dLdw for step() call\n",
    "        3. Return dLdx\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        self.dLdW = self.X.T.dot(dLdy)\n",
    "        self.dLdx = dLdy @ self.W.T\n",
    "        self.dLdb = np.sum(dLdy, axis=0)\n",
    "        return self.dLdx\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        '''\n",
    "        1. Apply gradient dLdw to network:\n",
    "        w <- w - l*dLdw\n",
    "        '''\n",
    "        self.W -= learning_rate * self.dLdW\n",
    "        self.b -= learning_rate * self.dLdb\n",
    "        #### YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2: Численный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Релизуйте функцию проверки численного градиента. Для этого для каждой переменной, по которой считается градиент, надо вычислить численный градиент: $f'(x) \\approx \\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$. Функция должна возвращать максимальное абсолютное отклонение аналитического градиента от численного. В качестве $\\epsilon$ рекомендуется взять $10^{-6}$. При правильной реализации максимальное отличие будет иметь порядок $10^{-8}-10^{-6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(func, X, gradient):\n",
    "    '''\n",
    "    Computes numerical gradient and compares it with analytcal.\n",
    "    func: callable, function of which gradient we are interested. Example call: func(X)\n",
    "    X: np.array of size (n x m)\n",
    "    gradient: np.array of size (n x m)\n",
    "    Returns: maximum absolute diviation between numerical gradient and analytical.\n",
    "    '''\n",
    "    #### YOUR CODE HERE\n",
    "    eps = 1e-5\n",
    "    func_grad = np.zeros(gradient.shape)\n",
    "    for i in range(func_grad.shape[0]):\n",
    "        for j in range(func_grad.shape[1]):\n",
    "            X[i, j] -= eps\n",
    "            func_l = func(X)\n",
    "            X[i, j] += 2*eps\n",
    "            func_r = func(X)\n",
    "            X[i, j] -= eps\n",
    "            func_grad[i, j] = (func_r - func_l) / (2 * eps)\n",
    "    return np.max(np.abs(gradient - func_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте линейный слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$ и $\\frac{\\partial L}{\\partial w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abs error for dLdx -  6.524589379885981e-08\n",
      "Abs error for dLdw -  6.443269739975221e-08\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE\n",
    "X = np.random.normal(0, 10, (10, 3))\n",
    "Y = np.random.normal(0, 20, (10, 2))\n",
    "l = Linear(3, 2)\n",
    "\n",
    "\n",
    "def loss_X(X):\n",
    "    res = l.forward(X)\n",
    "    return np.sum((res - Y) ** 2)\n",
    "\n",
    "def loss_W(W):\n",
    "    l.W = W\n",
    "    res = l.forward(X)\n",
    "    return np.sum((res - Y) ** 2)\n",
    "\n",
    "res = l.forward(X)\n",
    "dLdy = 2 * (res - Y) \n",
    "dLdx = l.backward(dLdy)\n",
    "\n",
    "print(\"Abs error for dLdx - \", check_gradient(loss_X, X, dLdx))\n",
    "print(\"Abs error for dLdw - \", check_gradient(loss_W, l.W, l.dLdW))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3: Сигмоида"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, d)\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layer to input\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = 1/(1 + np.exp(-X))\n",
    "        return self.Y\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdx.\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        self.dLdx = dLdy * (1/(np.exp(-self.X/2) + np.exp(self.X/2))**2)\n",
    "        return self.dLdx\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is abs error for gradient -  2.4961551492452827e-08\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE\n",
    "X = np.random.normal(10, 20, (10, 3))\n",
    "Y = np.random.normal(0, 10, (10, 3))\n",
    "\n",
    "s = Sigmoid()\n",
    "\n",
    "\n",
    "def loss_X_s(X):\n",
    "    res = s.forward(X)\n",
    "    return np.sum((res - Y)**2) \n",
    "\n",
    "res = s.forward(X)\n",
    "dLdy = 2*(res - Y)\n",
    "\n",
    "dLdx = s.backward(dLdy)\n",
    "\n",
    "print(\"This is abs error for gradient - \", check_gradient(loss_X_s, X, dLdx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4: Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы увидели на семинаре, вычисление производной для связки SoftMax + Negative log-likelihood проще чем для этих двух слоев по отдельности. Поэтому мы реализуем их как один класс. Важное замечание: на проходе \"вперед\" важно воспользоваться трюком <a href=\"https://blog.feedly.com/tricks-of-the-trade-logsumexp/\">log-sum-exp</a>, чтобы не столкнуться с численными неустойчивостями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Applies Softmax operation to inputs and computes NLL loss\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### (Hint: No code is expected here, just joking)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, C), where C is the number of classes\n",
    "        y is np.array of size (N), contains correct labels\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layer to input\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.NLLSoftMax = -X[np.arange(0, len(y)), y] + np.log(np.sum(np.exp(X), axis=1))\n",
    "        return self.NLLSoftMax\n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        Note that here dLdy = 1 since L = y\n",
    "        1. Compute dLdx\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        Exp = np.exp(X)\n",
    "        self.dLdx = ((Exp * X).T/ np.sum(Exp, axis=1)).T\n",
    "        self.dLdx[np.arange(0, len(self.y)), y] -= 1\n",
    "        return self.dLdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "X = np.random.normal(0, 10, (10, 3))\n",
    "y = np.array([0, 1, 2, 0, 1, 1, 0, 0, 0, 0])\n",
    "nll_loss = NLLLoss()\n",
    "\n",
    "nll_loss.forward(X, y)\n",
    "\n",
    "gradient = nll_loss.backward()\n",
    "\n",
    "def loss_X_nll(X):\n",
    "    return nll_loss.forward(X, y)\n",
    "\n",
    "print(\"This is abs error for gradient - \", check_gradient(loss_X_nll, ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 5, нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть \"кирпичики\", мы можем написать класс, который будет собирать всю сеть вместе "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, modules):\n",
    "        '''\n",
    "        Constructs network with *modules* as its layers\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layers to input\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        dLdy here is a gradient from loss function\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 6, обучение на простых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-286b9808ba39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'equal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/neural_networks/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.npz'"
     ]
    }
   ],
   "source": [
    "data = np.load('data.npz')\n",
    "X, y = data['arr_0'], data['arr_1']\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите архитектуру вида 2 -> 10 -> 10 -> 3:\n",
    "* Linear(2, 10)\n",
    "* Sigmoid()\n",
    "* Linear(10, 10)\n",
    "* Sigmoid()\n",
    "* Linear(10, 3)\n",
    "\n",
    "В качестве функции потерь используйте NLLLoss.\n",
    "1. Создайте сеть, в цикле запускайте forward, backward, step (используйте learning rate 0.005). \n",
    "2. Нарисуйте график сходимости (величина NLL после каждого обновления).\n",
    "3. Нарисуйте разделяющую поверхность\n",
    "4. Попробуйте подобрать темп обучения. Как меняется сходимость?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличная визуализация: http://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Подоберите темп обучения. Как меняется сходимость? Нарисуйте график оптимального значения функции потерь для различных значений learning_rate\n",
    "* Решите поставленную выше задачу как задачу регрессии с MSE. Изменилась ли разделяющая поверхность?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
